---
title: "Sei's drum!"
author: "Michael Großbach"
date: Draft `r Sys.Date()`
output:
  html_document: 
    fig_caption: yes
    toc: yes
    toc_float: yes
    code_folding: hide
    fig_width: 3.5
bibliography: asiams.bib
editor_options:
    chunk_output_type: console
---

```{r Libraries, message=FALSE}
library(brms)
library(dplyr)
library(ggplot2)
library(ggthemes)
library(kfigr)
library(knitr)
# library(pander)
library(patchwork)
# library(tidyr)
```

```{r Settings, echo=FALSE}
opts_chunk$set(comment = NULL,
               fig.width = 12,
               fig.height = 12, 
               echo = TRUE)
theme_set(theme_light())
knit_hooks$set(anchor = kfigr::hook_anchor)
opts_knit$set(kfigr.link = TRUE,
              kfigr.prefix = TRUE)
knitr::opts_chunk$set(fig.width = 3.5,
                      fig.height = 3.5)
options(mc.cores = parallel::detectCores())
rstan::rstan_options(auto_write = TRUE)
# Constants:
MODEL <- FALSE
FIGREF <- "Fig." # type for pics and plots, kfigr::figr()
TABREF <- "Tab." # type for tables, kfigr::figr()
MODREF <- "Model" # type for models, kfigr::figr()
PRIORREF <- "Prior" # type for priors, kfigr::figr()
SRCREF <- "Src." # type for source code blocks, kfigr::figr()
COLOR_BLIND_PALETTE <- RColorBrewer::brewer.pal(n = 9, name = "YlOrRd")
source("dahl_drumming_models.R")
```

The data set was read in as is, the columns `subj`, `arm`, `cond`, `series` and `no` were converted to factors, the column `filename` was deleted.

```{r}
read.csv("features.csv") %>%
  mutate(filename = NULL,
         series = factor(series),
         subj = factor(subj),
         arm = factor(arm),
         cond = factor(cond),
         no = factor(no)) ->
  drum_beats
```

# Exploration

In a first attempt to get an overview, the descriptors' density estimates were plotted, broken down by `arm` and `cond`.

```{r DescriptorDensities, anchor=FIGREF, fig.width=12, fig.height=18}
dep_vars <- names(drum_beats)[which(!(names(drum_beats) %in% c("subj", 
                                                               "arm", 
                                                               "cond", 
                                                               "no",
                                                               "series")))]
n_vars <- length(dep_vars)
  plot_lst <- vector("list", length = n_vars)
  plt_cnt <- 1
  for (dv in dep_vars) {
    if (plt_cnt == n_vars) {
      plt_pos <- c(2, 0.5)
    } else {
      plt_pos <- "none"
    }
    if (plt_cnt %% 4 == 1) {
      ylab <- "density"
    } else {
      ylab <- ""
    }
    plot_lst[[plt_cnt]] <- ggplot(drum_beats, 
                                  aes_string(x = dv, 
                                             color = "cond")) +
      geom_density(na.rm = TRUE) + 
      scale_color_colorblind() +
      ylab(ylab) +
      geom_rug(alpha = 0.25) + 
      theme(legend.position = plt_pos) +
      facet_wrap(~ arm)
    plt_cnt <- plt_cnt + 1
  }
  print(wrap_plots(plot_lst, 
                   ncol = 4) +
          plot_annotation(
            tag_levels = "A"))
```

`r figr("DescriptorDensities", type = FIGREF)`. *Descriptor densities along with raw data points (rug ticks on x axes), broken down by arm (dominant [D] vs non-dominant [ND]) and instruction condition (controlled = C, normal = N).*

***

Several peculiarities are immediately apparent from the plots in `r figr("DescriptorDensities")`:

* There are no substantial differences in the density distributions between normal (N) and controlled (C) strokes
* <strike>`attDur`, `LAT`, and `attFlat` have very few unique data values (see rug ticks at bottom of plots)</strike>
* Looking at the dominant arm alone <strike>(and excluding the vars `attDur`, `LAT`, and `attFlat` for the reason just mentioned)</strike>, the densities have a trend toward a larger variance in the controlled condition (in other words: a less pointed distribution) with additional or larger humps in the tails, respectively

The lack of any clear-cut differences in the raw data between conditions suggests that it will be hard to find any differences by modeling.

<strike>The limited number of unique values in the attack-related measures (*e.g.* `attDur` only has `r length(unique(drum_beats$attDur))` unique values in a total of `r nrow(drum_beats)` observations across all subjects, conditions, and sides; that's only `r round(length(unique(drum_beats$attDur)) / nrow(drum_beats) * 100)` percent!) suggests that rounding errors propagated through the calculations. This is probably due to very similar, *i.e.* highly automated, and short attack times combined with the given sampling frequency, resulting in few data points, which in course of the calculations result in the observed phenomenon. But I'm just guessing here, as I do not know anything about these descriptors and how they are calculated or interpreted. Judging by the range of values of, *i.e.*, `attDur` (`r round(range(drum_beats$attDur), 2)`ms, across both sides and conditions) and a sampling frequency of 48 kHz, this leaves us at 48 * (`r round(range(drum_beats$attDur), 2)[2]` - `r round(range(drum_beats$attDur), 2)[1]`) = `r round(48 * (range(drum_beats$attDur)[2] - range(drum_beats$attDur)[1]))` points to choose beginning and end of the attack. Given the supposed highly automatized motor program used to initiate the stroke, along with the laws of physics at play here (no pun intended), it is not very surprising to see very few unique values. My limited knowledge---or rather, my ignorance of anything sound-related---aside, from a statistical standpoint these measures do not seem suited to describe any differences between the experimental conditions investigated here.</strike> 

<strike>Given the very few data points in the attack phase, any descriptor derived from such a short period of time (*.e.g.*. `attSPL`) cannot be judged as being stable in the sense of being reproducible. Hence I suggest to drop all attack-derived descriptors.</strike>

```{r}
# drum_beats %>%
#   mutate(attDur = NULL,
#          LAT = NULL,
#          attSPL = NULL,
#          attSC = NULL,
#          attFlat = NULL,
#          attSpecFlat = NULL) ->
#   drum_beats
```

The increase in variance for the conditions N < C does not come as a surprise as I assume normal also means highly trained and thus automatized, whereas controlled involves less automatization and more 'individualness' both within and between subjects. 

## Conclusions

Looking at the above investigated descriptors and @Danielsen2015, it seems reasonable to limit the modeling attempts to `totDur`, `totSPL`, `totSC`, and `TC`. 

But Sofia wrote on 2020-07-09: "Francesco and I have discussed a bit related to descriptors and we want to concentrate on the “transient” period (although the name probably will change). Spectral Centroid (transSC) should be one, and I suggest transFlat for the other. Francesco, does that sound reasonable?"  

On 2020-07-22 both Sofia and Francesco agreed upon `transSC`, `transFlat`, and `transCrest` as the probably most important response variables to look at.

## Francesco's comments

Update: 

* The lack of data points for `attFlat` is fixed by solving a bug in the feature extraction: now the downsampling ratio for the envelope extraction is reduced, and makes the *MIRToolbox* algorithm less sensitive to frames with an rms value of 0 (which returns an incorrect value of 0, since we have a geometric mean at the numerator). Now we have a larger spread which allows high values (\sim 1 = peaky envelope, \sim 0 = smooth/flat envelope).
* The crest factor is now calculated for the separate phases.

Regarding the attack phase: I agree with the remarks regarding duration. Given the fact that we are not comparing different instruments, I wouldn't have expected a large variation. This is not surprising if we consider that our system is changing only slightly (same rototom, same drumstick, same action, a bit different tuning across subjects): in fact, the perceived timbral differences are so small that we are in trouble guessing on the descriptors.

The sampling frequency is even lower ($f_s = 44100$ Hz). Even if we had a higher sample rate which could reveal some discrepancies in the attack durations, we would have to prove that they are perceptually relevant.

Therefore, I am happy to discard `attDur` and `LAT` (which is obviously a log-transformed duration, only there for the sake of consistency with the literature).

I am a bit more in doubt when it comes to discarding all the attack descriptors. Even if what Michael says is true from a statistical point of view, we should still be able to discriminate timbre on short time windows due to the high temporal resolution of our hearing. Attack phase descriptors (with the same definition of attack that we are using, which is most likely not coincident with perceptual attack) are employed in @camara_effects_2020, and the Oslo group has a paper under construction which analyzes drum sounds in a similar manner (see *OsloPlots.png* in the OneDrive folder).

I am worried that merely taking the overall descriptors into account would introduce a lot of unnecessary and perceptually catching information --- mostly the tonal part of the signal, i.e. the drum ringing in the last part of the decay. That's why Sofia and I are suggesting to look at what we could call "main energy" or "early decay" phase (i.e. from max peak to temporal centroid).

Would it be feasible to set up 4 different models (i.e. one for each phase), at least in the univariate version?

As for the descriptors to include: although `TC` is employed in @Danielsen2015, the PDFs are even more similar. I would go for `Dur` (except attack?), `SPL`, `SC`, and one between `Flat`, `SpecFlat` or `Crest`.

My (informal and biased) listening tests tell me that, at least for some subjects, I hear a pattern going towards a harsher (controlled) vs smoother (normal) timbre exactly at the hit point, plus slightly less (controlled) or more pitch/amplitude fluctuation. Hopefully this could be catched by spectral centroid, specrtral/temporal flatness, or crest factor. This should be independent of SPL unless the subject misinterpreted the instructions, therefore SPL acts as a sort of control variable in our model.  
>>>>>>> ee85c644e2e9174cb164e81a3e7092b9f049ee82

# Model Considerations

There are several points that need to be considered before making a decision regarding the type of modeling to be done in this study, (1) the sample size places restrictions on the external validity; (2) data from small samples can be better modeled when regularization is in place to 'tame' the estimates; (3) the hierarchical structure of the data (subjects played several trials with either their dominant or non-dominant arm under two conditions) suggests a multilevel analysis of the data which would, in addition to the Bayesian regularization via priors also results in shrinkage of the estimates; (4) given the small distribution differences between the two experimental conditions in the varying variance between conditions

Given the extremely small sample size, any attempt to fit the data using traditional statistics, a.k.a. null-hypothesis significance testing, would make us vulnerable to all kinds of criticism. I therefor suggest to use Bayesian statistics instead, as it allows us to estimate probability distributions of parameters rather than confidence intervals around point estimates, and thus embraces uncertainty in estimates. 

Additionally, Bayesian regression uses prior probability distributions to arrive at sensible estimates. These priors regularize estimates, or draw them toward zero, a desirable effect which has long been recognized even in traditional statistics (*e.g.* ridge regression and lasso; @Tibshirani1996).

ANOVA (or regression with grouping variables) requires each subject's trials to be averaged, say, within conditions or groups (or both), to be able to assess treatment effects; this results in the individual variation within a subject to get lost while it might have been very informative to include it in the analysis. Multilevel (or hierarchical) modeling allows to include the entire data structure (`r figr("MLMstructure", type = FIGREF)`) so that no information gets lost through averaging, but all variation (both individual and treatment-driven) propagates through the analysis and ends up in the final results, allowing for more realistic credibility margins around estimates. 

```{r MLMstructure, anchor=FIGREF}
# chunk intentionally empty  
```

~~~
Instruction:    Normal        			Controlled
                | |    \      			/ |    \
Player:         1 2 ... n			   1  2 ... n
               / \
Side   dominant non-dominant
        / | \	    / | \
Trial  1 ..  p     1  .. p
~~~

`r figr("MLMstructure")`. *The multilevel structure of a data set should be reflected in the analysis.*

***

Also, the variation in traditional statistics is considered to be fixed (think homoscedasticity in ANOVA, or the $\epsilon$ in regression formulas such as $y_i = \beta_0 + \beta_1x_i + \epsilon_i$), whereas it is an estimated quantity in Bayesian statistics and therefor can vary between conditions (or groups or subjects etc.). 

## Conclusions

Will start with a simple univariate model, add predictors and interactions, then a bivariate model, and finally a quadruple-variate model and see where this leads us.

# Modeling

Sofia, on 2020-07-07: "The main hypothesis is that playing instruction (N/C) will affect the stroke in a way that is perceivable".

## Simple Univariate Models

Looking again at the variables agreed upon <strike>*I* have decided</strike> to use as response variables in the regression models<strike>---implying that this is by no means set in stone---</strike>, it seems like a skewed normal link function would be appropriate to model them.

Let's start with `transSC`<strike>`totDur`</strike>. Using an extended model description language [@Bates2010;@Buerkner2018], going back to Wilkinson and Rogers's [-@Wilkinson1973] modeling language, we write:

```{r}
(m0_form <-bf(transSC ~ 1 + (1 | subj)))
```

which claims that `transSC` is explained by ('~') an intercept, denoted by '1', and an additional term '(1 | subj)'. The '1' in parentheses again stands for the intercept, but the pipe '|' assigns an intercept to each level of the factor 'subj'. In this particular case this means that the model will estimate an individual intercept for each unique drummer listed in the data set column `subj`. These individual, or varying, intercepts are then used in informing the estimation of the population intercept.

Note: set `MODEL` to `TRUE` at the top of the script if you didn't compile/build your model yet.

```{r}
if (MODEL) {
  m0 <- brm(m0_form,
            family = skew_normal(),
            data = drum_beats)
  m0 <- add_criterion(m0, 
                      "loo",
                      reloo = TRUE)
  save(m0, 
       file = "m0.rda")
} else {
  load("m0.rda")
}
```

This null model is also termed unconditional model because it has no grouping structure apart from individuals--the 'subj' bit in the model equation above. There is some variation in every natural data set. To make sure, it's not just variation caused by different participants, we can calculate the intra-class correlation coefficient (ICC).

```{r}
m0_icc <- ICC(m0, "subj")
```

The Null model's ICC amounts to `r round(m0_icc, 2)`, which suggests that approx. `r round(m0_icc, 2) * 100` percent of the variation in the data set can be attributed to (or explained by) the grouping structure. This is highly unfortunate, as it does not leave a lot of variation to be explained by independent factors like instruction, or arm. The most likely reason for this high ICC value is the small sample size combined with high inter-individual variation. Small sample sizes combined with large trial numbers are less of a problem when subjects respond, on average, close to the population mean, even with large spread due to fluctuating alertness, increasing fatigue etc., *.e.g.* in reaction time paradigms. But here, with large intra- *and* inter-individual variation, this might become a problem.

***

`r figr("m0_summary", type = TABREF)`. *Model summary.*

```{r m0_summary, achor = TABREF}
(m0_summary <- summary(m0, 
                       priors = TRUE))
```

The intercept in `r figr("m0_summary")` amounts to roughly `r round(m0_summary$fixed["Intercept", "Estimate"])`. In this model specification, the intercept is identical to the data set average. The table also shows that the inter-individual standard deviation (`sd(Intercept)`) is large compared to the unexplained variation (`sigma`). This led to the large ICC value above. By adding more and more independent factors to the model specification, we will later try to decrease $\sigma$, *i.e.* 'explain away' as much remaining variation as possible. 

```{r m0_ppcheck, anchor=FIGREF}
pp_check(m0, nsamples = 100)
```

`r figr("m0_ppcheck", type = FIGREF)`. *Posterior predictive check. The thick blue line shows the distribution of the empirical data. The thin blue lines are one-hundred realizations of data generated from parameters estimated by the model.*

***

The posterior predictive plot in `r figr("m0_ppcheck")` gives an impression on how the null model would generate data, given the parameters it estimated from the empirical data. As the thick line deviates from the modeled thin lines, especially on the left and the right side of the peak of the distribution, it is apparent that the model can be improved.

```{r}
# conditional_effects(m0) 
```

## Condition Model

In this model, the intercept is complemented with second 'main' or population effect, the instruction condition `cond`:

```{r}
(m1_form <-bf(transSC ~ 1 + cond + (1 | subj)))
```

I could have also specified the model without the explicit '1 + ', as the intercept is implicitly included in the model unless I explicitly exclude it. From now on I will always save some extra typing by refraining from explicitly indluding the intercept in models.

So now the model not only contains the individuals as grouping structure to 'explain away' variation, but also the manipulations.

The prior distributions in Bayesian models reflect the knowledge about the estimated parameters. The package `brms` automatically places weakly informative priors on parameters as soft contraints. But with more complex models involving varying parameter estimates, the statistical back end which does the heavy lifting, needs stronger priors particularly on these parameters, otherwise models don't converge. The parameters most vulnerable to outliers in the data are the varying effects parameters, or random effects in traditional statistics. Therefor we place a stronger prior probability distribution over the estimate of the SD of individual intercepts:

```{r}
(m1_prior <- set_prior("normal(0, 10)", class = "sd"))
```

and leave the rest of the priors as suggested by the package `brms` (see `r figr("m1_summary", type = TABREF)` for their priors).

```{r}
if (MODEL) {
  m1 <- brm(m1_form,
            family = skew_normal(),
            data = drum_beats)
  m1 <- add_criterion(m1, 
                      "loo",
                      reloo = TRUE)
  save(m1, 
       file = "m1.rda")
} else {
  load("m1.rda")
}
```

***

`r figr("m1_summary")`. *Model summary.*

```{r m1_summary, achor = TABREF}
(m1_summary <- summary(m1, 
                       priors = TRUE))
```

The intercept in `r figr("m1_summary")` amounts to roughly `r round(m1_summary$fixed["Intercept", "Estimate"])`. In this model specification, the intercept is identical to the controlled strokes, while the `condN` value is the mean of the posterior distribution *difference* between controlled and normal strokes (`r round(m1_summary$fixed["condN", "Estimate"])`). The table also shows that the interindividual standard deviation (`sd(Intercept)` $\approx$ `r round(m1_summary$random$subj["sd(Intercept)", "Estimate"])` is still large compared to the unexplained variation (`sigma` $\approx$ `r round(m1_summary$spec_pars["sigma", "Estimate"])`). 

```{r m1_ppcheck, anchor=FIGREF}
pp_check(m1, nsamples = 100)
```

`r figr("m1_ppcheck", type = FIGREF)`. *Posterior predictive check. The thick blue line shows the distribution of the empirical data. The thin blue lines are one-hundred realizations of data generated from parameters estimated by the model.*

***

The posterior predictive plot in `r figr("m1_ppcheck")` gives an impression on how this model would generate data, given the parameters it estimated from the empirical data. There is not much change from `r figr("m0_ppcheck")`, which is not very surprising given the small estimated difference between the conditions.

```{r}
conditional_effects(m1)
```

## ConditionVI Model

The last model included the instruction condition, more realistically reflecting the true structure of the data set. But it did not acknowledge that each subject executed several strokes in each of these conditions. This model includes a term with a varying intercept (VI) for condition to reflect just that, which will also assist in more realistically estimate the population effect of `cond`:

```{r}
(m2_form <-bf(transSC ~ cond + 
                (1 | subj) + 
                (1 | cond)))
```

Including more varying parameters in the model requires the prior on them to be even stronger:

```{r}
(m2_prior <- set_prior("normal(0, 5)", class = "sd"))
```

```{r}
if (MODEL) {
  m2 <- brm(m2_form,
            prior = m2_prior,
            family = skew_normal(),
            data = drum_beats,
            control = list(adapt_delta = 0.95))
  m2 <- add_criterion(m2, 
                      "loo",
                      reloo = TRUE)
  save(m2, 
       file = "m2.rda")
} else {
  load("m2.rda")
}
```

***

`r figr("m2_summary", type = TABREF)`. *Model summary.*

```{r m2_summary, achor = TABREF}
(m2_summary <- summary(m2, 
                       priors = TRUE))
```

The intercept in `r figr("m2_summary")` amounts to roughly `r round(m2_summary$fixed["Intercept", "Estimate"])`. In this model specification, the intercept is identical to the controlled strokes, while the `condN` value is the mean of the posterior distribution *difference* between controlled and normal strokes (`r round(m2_summary$fixed["condN", "Estimate"])`). The table also shows that the inter-individual standard deviation (`sd(Intercept)` $\approx$ `r round(m2_summary$random$subj["sd(Intercept)", "Estimate"])` is not large anymore compared to the unexplained variation (`sigma` $\approx$ `r round(m2_summary$spec_pars["sigma", "Estimate"])`). 

```{r m2_ppcheck, anchor=FIGREF}
pp_check(m2, nsamples = 100)
```

`r figr("m2_ppcheck", type = FIGREF)`. *Posterior predictive check. The thick blue line shows the distribution of the empirical data. The thin blue lines are one-hundred realizations of data generated from parameters estimated by the model.*

***

The posterior predictive plot in `r figr("m2_ppcheck")` gives an impression on how this model would generate data, given the parameters it estimated from the empirical data. There is not much change from `r figr("m0_ppcheck")`, which is not very surprising given the small estimated difference between the conditions.

```{r}
conditional_effects(m2)
```

# System Setup

Data wrangling and analyses were carried out with the statistical package `R` [`r version$version.string`; @RCT2020]. Bayesian modeling was done with the package `brms` [@Buerkner2017;@Buerkner2018] which uses the probabilistic language Stan as back end [@Carpenter2017]. Plots were done with the packages `bayesplot` [@Gabry2020] and `ggplot2` [@Wickham2016].

# References 
